{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import voting_classifier as vc\n",
    "data=pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=data[data.target ==0]\n",
    "\n",
    "\n",
    "\n",
    "t1=t0.iloc[:5000]\n",
    "t2=t0.iloc[22375:27375]\n",
    "t3=t0.iloc[49750:54750]\n",
    "t4=t0.iloc[75125:80125]\n",
    "t5=t0.iloc[94500:99500]\n",
    "t6=t0.iloc[116875:121875]\n",
    "t7=t0.iloc[144250:149250]\n",
    "t8=t0.iloc[166625:171625]\n",
    "\n",
    "\n",
    "Data=t1.append(t2)\n",
    "Data=Data.append(t3)\n",
    "Data=Data.append(t4)\n",
    "Data=Data.append(t5)\n",
    "Data=Data.append(t6)\n",
    "Data=Data.append(t7)\n",
    "Data=Data.append(t8)\n",
    "t9=data[data.target ==1]\n",
    "Data=Data.append(t9)\n",
    "#Data=Data.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data[(data > 0)].all(1):\n",
    "        Data=data\n",
    "else:\n",
    "        Data=0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, data.columns !='target']\n",
    "X = X.loc[:, X.columns !='ID_code']\n",
    "y = data.loc[:, data.columns == 'target']\n",
    "\n",
    "col=['var_7','var_39','var_14','var_46','var_79','var_129','var_124','var_136','var_161','var_153','var_10','var_17','var_27','var_30','var_38','var_41','var_96','var_98','var_100','var_103','var_117','var_126','var_158','var_160','var_183','var_185']\n",
    "#X.drop(col,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "xTrain,xTest,yTrain,yTest=tts(X,y,stratify=y,test_size=0.33,random_state=42)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler as ss\n",
    "scalar=ss()\n",
    "xTrainS=scalar.fit(xTrain).fit_transform(xTrain)\n",
    "xTestS=scalar.fit(xTest).fit_transform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40265 samples, validate on 19833 samples\n",
      "Epoch 1/100\n",
      "40265/40265 [==============================] - 8s 201us/step - loss: 0.4664 - acc: 0.7795 - val_loss: 0.4554 - val_acc: 0.7934\n",
      "Epoch 2/100\n",
      "40265/40265 [==============================] - 9s 220us/step - loss: 0.4185 - acc: 0.8092 - val_loss: 0.4495 - val_acc: 0.7929\n",
      "Epoch 3/100\n",
      "40265/40265 [==============================] - 9s 214us/step - loss: 0.3959 - acc: 0.8205 - val_loss: 0.4584 - val_acc: 0.7906\n",
      "Epoch 4/100\n",
      "40265/40265 [==============================] - 9s 223us/step - loss: 0.3737 - acc: 0.8351 - val_loss: 0.4705 - val_acc: 0.7872acc: \n",
      "Epoch 5/100\n",
      "40265/40265 [==============================] - 12s 296us/step - loss: 0.3514 - acc: 0.8474 - val_loss: 0.4877 - val_acc: 0.7777\n",
      "Epoch 6/100\n",
      "40265/40265 [==============================] - 12s 286us/step - loss: 0.3289 - acc: 0.8589 - val_loss: 0.5127 - val_acc: 0.7746\n",
      "Epoch 7/100\n",
      "40265/40265 [==============================] - 11s 279us/step - loss: 0.3070 - acc: 0.8710 - val_loss: 0.5315 - val_acc: 0.7717\n",
      "Epoch 8/100\n",
      "40265/40265 [==============================] - 10s 252us/step - loss: 0.2861 - acc: 0.8822 - val_loss: 0.5575 - val_acc: 0.7641\n",
      "Epoch 9/100\n",
      "40265/40265 [==============================] - 11s 277us/step - loss: 0.2670 - acc: 0.8914 - val_loss: 0.5878 - val_acc: 0.7615\n",
      "Epoch 10/100\n",
      "40265/40265 [==============================] - 10s 259us/step - loss: 0.2491 - acc: 0.8992 - val_loss: 0.6172 - val_acc: 0.7600\n",
      "Epoch 11/100\n",
      "40265/40265 [==============================] - 11s 269us/step - loss: 0.2327 - acc: 0.9067 - val_loss: 0.6485 - val_acc: 0.7571\n",
      "Epoch 12/100\n",
      "40265/40265 [==============================] - 11s 263us/step - loss: 0.2154 - acc: 0.9153 - val_loss: 0.6895 - val_acc: 0.7544\n",
      "Epoch 13/100\n",
      "40265/40265 [==============================] - 10s 257us/step - loss: 0.2034 - acc: 0.9196 - val_loss: 0.7092 - val_acc: 0.7503\n",
      "Epoch 14/100\n",
      "40265/40265 [==============================] - 10s 252us/step - loss: 0.1878 - acc: 0.9286 - val_loss: 0.7538 - val_acc: 0.7467\n",
      "Epoch 15/100\n",
      "40265/40265 [==============================] - 10s 256us/step - loss: 0.1766 - acc: 0.9326 - val_loss: 0.7869 - val_acc: 0.7429\n",
      "Epoch 16/100\n",
      "40265/40265 [==============================] - 12s 293us/step - loss: 0.1651 - acc: 0.9386 - val_loss: 0.8293 - val_acc: 0.7500\n",
      "Epoch 17/100\n",
      "40265/40265 [==============================] - 12s 309us/step - loss: 0.1542 - acc: 0.9431 - val_loss: 0.8619 - val_acc: 0.7418\n",
      "Epoch 18/100\n",
      "40265/40265 [==============================] - 12s 298us/step - loss: 0.1453 - acc: 0.9475 - val_loss: 0.8869 - val_acc: 0.7438\n",
      "Epoch 19/100\n",
      "40265/40265 [==============================] - 12s 287us/step - loss: 0.1351 - acc: 0.9503 - val_loss: 0.9250 - val_acc: 0.7436\n",
      "Epoch 20/100\n",
      "40265/40265 [==============================] - 11s 271us/step - loss: 0.1268 - acc: 0.9552 - val_loss: 0.9666 - val_acc: 0.7451\n",
      "Epoch 21/100\n",
      "40265/40265 [==============================] - 10s 261us/step - loss: 0.1210 - acc: 0.9572 - val_loss: 1.0009 - val_acc: 0.738394 - acc\n",
      "Epoch 22/100\n",
      "40265/40265 [==============================] - 11s 262us/step - loss: 0.1129 - acc: 0.9613 - val_loss: 1.0325 - val_acc: 0.7432\n",
      "Epoch 23/100\n",
      "40265/40265 [==============================] - 10s 259us/step - loss: 0.1101 - acc: 0.9606 - val_loss: 1.0782 - val_acc: 0.7394\n",
      "Epoch 24/100\n",
      "40265/40265 [==============================] - 10s 258us/step - loss: 0.1019 - acc: 0.9648 - val_loss: 1.1178 - val_acc: 0.7323\n",
      "Epoch 25/100\n",
      "40265/40265 [==============================] - 10s 259us/step - loss: 0.0943 - acc: 0.9678 - val_loss: 1.1448 - val_acc: 0.7395\n",
      "Epoch 26/100\n",
      "40265/40265 [==============================] - 10s 241us/step - loss: 0.0913 - acc: 0.9682 - val_loss: 1.1730 - val_acc: 0.7327los\n",
      "Epoch 27/100\n",
      "40265/40265 [==============================] - 10s 259us/step - loss: 0.0892 - acc: 0.9695 - val_loss: 1.2051 - val_acc: 0.7341\n",
      "Epoch 28/100\n",
      "40265/40265 [==============================] - 10s 260us/step - loss: 0.0828 - acc: 0.9724 - val_loss: 1.2237 - val_acc: 0.7369s - loss: 0.0802\n",
      "Epoch 29/100\n",
      "40265/40265 [==============================] - 10s 244us/step - loss: 0.0834 - acc: 0.9710 - val_loss: 1.2794 - val_acc: 0.7350\n",
      "Epoch 30/100\n",
      "40265/40265 [==============================] - 10s 254us/step - loss: 0.0753 - acc: 0.9745 - val_loss: 1.3079 - val_acc: 0.7338\n",
      "Epoch 31/100\n",
      "40265/40265 [==============================] - 10s 253us/step - loss: 0.0773 - acc: 0.9720 - val_loss: 1.3268 - val_acc: 0.7352\n",
      "Epoch 32/100\n",
      "40265/40265 [==============================] - 10s 255us/step - loss: 0.0721 - acc: 0.9758 - val_loss: 1.3487 - val_acc: 0.7372s - los - ETA: 1s -\n",
      "Epoch 33/100\n",
      "40265/40265 [==============================] - 10s 255us/step - loss: 0.0724 - acc: 0.9750 - val_loss: 1.3850 - val_acc: 0.7304\n",
      "Epoch 34/100\n",
      "40265/40265 [==============================] - 10s 248us/step - loss: 0.0702 - acc: 0.9766 - val_loss: 1.3985 - val_acc: 0.7340\n",
      "Epoch 35/100\n",
      "40265/40265 [==============================] - 10s 255us/step - loss: 0.0647 - acc: 0.9787 - val_loss: 1.4255 - val_acc: 0.7347\n",
      "Epoch 36/100\n",
      "40265/40265 [==============================] - 10s 258us/step - loss: 0.0673 - acc: 0.9770 - val_loss: 1.4472 - val_acc: 0.7355\n",
      "Epoch 37/100\n",
      "40265/40265 [==============================] - 10s 245us/step - loss: 0.0639 - acc: 0.9777 - val_loss: 1.4634 - val_acc: 0.7400\n",
      "Epoch 38/100\n",
      "40265/40265 [==============================] - 10s 259us/step - loss: 0.0646 - acc: 0.9782 - val_loss: 1.4902 - val_acc: 0.7326\n",
      "Epoch 39/100\n",
      "40265/40265 [==============================] - 10s 248us/step - loss: 0.0620 - acc: 0.9787 - val_loss: 1.5018 - val_acc: 0.7359\n",
      "Epoch 40/100\n",
      "40265/40265 [==============================] - 10s 254us/step - loss: 0.0583 - acc: 0.9806 - val_loss: 1.5444 - val_acc: 0.7385\n",
      "Epoch 41/100\n",
      "40265/40265 [==============================] - 7s 168us/step - loss: 0.0599 - acc: 0.9794 - val_loss: 1.5555 - val_acc: 0.7367\n",
      "Epoch 42/100\n",
      "40265/40265 [==============================] - 7s 178us/step - loss: 0.0602 - acc: 0.9789 - val_loss: 1.5705 - val_acc: 0.7329\n",
      "Epoch 43/100\n",
      "40265/40265 [==============================] - 7s 186us/step - loss: 0.0541 - acc: 0.9826 - val_loss: 1.5931 - val_acc: 0.7347\n",
      "Epoch 44/100\n",
      "40265/40265 [==============================] - 7s 182us/step - loss: 0.0567 - acc: 0.9804 - val_loss: 1.6214 - val_acc: 0.7346\n",
      "Epoch 45/100\n",
      "40265/40265 [==============================] - 6s 153us/step - loss: 0.0605 - acc: 0.9792 - val_loss: 1.6130 - val_acc: 0.7351\n",
      "Epoch 46/100\n",
      "40265/40265 [==============================] - 6s 141us/step - loss: 0.0584 - acc: 0.9799 - val_loss: 1.6250 - val_acc: 0.7285\n",
      "Epoch 47/100\n",
      "40265/40265 [==============================] - 6s 137us/step - loss: 0.0511 - acc: 0.9835 - val_loss: 1.6407 - val_acc: 0.7367\n",
      "Epoch 48/100\n",
      "40265/40265 [==============================] - 5s 136us/step - loss: 0.0538 - acc: 0.9818 - val_loss: 1.6465 - val_acc: 0.7347\n",
      "Epoch 49/100\n",
      "40265/40265 [==============================] - 6s 141us/step - loss: 0.0534 - acc: 0.9814 - val_loss: 1.6719 - val_acc: 0.7333\n",
      "Epoch 50/100\n",
      "40265/40265 [==============================] - 6s 161us/step - loss: 0.0552 - acc: 0.9805 - val_loss: 1.6859 - val_acc: 0.7297\n",
      "Epoch 51/100\n",
      "40265/40265 [==============================] - 5s 136us/step - loss: 0.0524 - acc: 0.9822 - val_loss: 1.7026 - val_acc: 0.7320\n",
      "Epoch 52/100\n",
      "40265/40265 [==============================] - 5s 134us/step - loss: 0.0518 - acc: 0.9828 - val_loss: 1.7077 - val_acc: 0.7348\n",
      "Epoch 53/100\n",
      "40265/40265 [==============================] - 5s 132us/step - loss: 0.0499 - acc: 0.9829 - val_loss: 1.7233 - val_acc: 0.7332\n",
      "Epoch 54/100\n",
      "40265/40265 [==============================] - 5s 136us/step - loss: 0.0510 - acc: 0.9818 - val_loss: 1.7415 - val_acc: 0.7263\n",
      "Epoch 55/100\n",
      "40265/40265 [==============================] - 6s 161us/step - loss: 0.0527 - acc: 0.9816 - val_loss: 1.7672 - val_acc: 0.7296\n",
      "Epoch 56/100\n",
      "40265/40265 [==============================] - 8s 194us/step - loss: 0.0509 - acc: 0.9822 - val_loss: 1.7469 - val_acc: 0.7290\n",
      "Epoch 57/100\n",
      "40265/40265 [==============================] - 5s 131us/step - loss: 0.0455 - acc: 0.9850 - val_loss: 1.7561 - val_acc: 0.7347\n",
      "Epoch 58/100\n",
      "40265/40265 [==============================] - 6s 156us/step - loss: 0.0548 - acc: 0.9808 - val_loss: 1.7717 - val_acc: 0.7300\n",
      "Epoch 59/100\n",
      "40265/40265 [==============================] - 7s 165us/step - loss: 0.0524 - acc: 0.9819 - val_loss: 1.7848 - val_acc: 0.7295\n",
      "Epoch 60/100\n",
      "40265/40265 [==============================] - 7s 170us/step - loss: 0.0442 - acc: 0.9853 - val_loss: 1.7819 - val_acc: 0.7333\n",
      "Epoch 61/100\n",
      "40265/40265 [==============================] - 7s 186us/step - loss: 0.0489 - acc: 0.9825 - val_loss: 1.7858 - val_acc: 0.7310\n",
      "Epoch 62/100\n",
      "40265/40265 [==============================] - 7s 164us/step - loss: 0.0460 - acc: 0.9846 - val_loss: 1.7963 - val_acc: 0.7317\n",
      "Epoch 63/100\n",
      "40265/40265 [==============================] - 7s 164us/step - loss: 0.0509 - acc: 0.9827 - val_loss: 1.8097 - val_acc: 0.7336\n",
      "Epoch 64/100\n",
      "40265/40265 [==============================] - 7s 167us/step - loss: 0.0494 - acc: 0.9828 - val_loss: 1.8201 - val_acc: 0.7314\n",
      "Epoch 65/100\n",
      "40265/40265 [==============================] - 7s 175us/step - loss: 0.0468 - acc: 0.9843 - val_loss: 1.8241 - val_acc: 0.7327\n",
      "Epoch 66/100\n",
      "40265/40265 [==============================] - 7s 165us/step - loss: 0.0437 - acc: 0.9850 - val_loss: 1.8516 - val_acc: 0.7309\n",
      "Epoch 67/100\n",
      "40265/40265 [==============================] - 8s 203us/step - loss: 0.0459 - acc: 0.9850 - val_loss: 1.8274 - val_acc: 0.7315\n",
      "Epoch 68/100\n",
      "40265/40265 [==============================] - 7s 180us/step - loss: 0.0447 - acc: 0.9843 - val_loss: 1.8558 - val_acc: 0.7327\n",
      "Epoch 69/100\n",
      "40265/40265 [==============================] - 8s 189us/step - loss: 0.0461 - acc: 0.9841 - val_loss: 1.8518 - val_acc: 0.7326\n",
      "Epoch 70/100\n",
      "40265/40265 [==============================] - 6s 159us/step - loss: 0.0470 - acc: 0.9838 - val_loss: 1.8786 - val_acc: 0.7306\n",
      "Epoch 71/100\n",
      "40265/40265 [==============================] - 8s 204us/step - loss: 0.0441 - acc: 0.9858 - val_loss: 1.8877 - val_acc: 0.7303\n",
      "Epoch 72/100\n",
      "40265/40265 [==============================] - 9s 214us/step - loss: 0.0460 - acc: 0.9841 - val_loss: 1.8817 - val_acc: 0.7350\n",
      "Epoch 73/100\n",
      "40265/40265 [==============================] - 7s 170us/step - loss: 0.0436 - acc: 0.9847 - val_loss: 1.8870 - val_acc: 0.7341\n",
      "Epoch 74/100\n",
      "40265/40265 [==============================] - 7s 174us/step - loss: 0.0483 - acc: 0.9830 - val_loss: 1.8900 - val_acc: 0.7328\n",
      "Epoch 75/100\n",
      "40265/40265 [==============================] - 7s 177us/step - loss: 0.0448 - acc: 0.9846 - val_loss: 1.9076 - val_acc: 0.7301\n",
      "Epoch 76/100\n",
      "40265/40265 [==============================] - 6s 160us/step - loss: 0.0404 - acc: 0.9858 - val_loss: 1.8985 - val_acc: 0.7321\n",
      "Epoch 77/100\n",
      "40265/40265 [==============================] - 6s 152us/step - loss: 0.0452 - acc: 0.9851 - val_loss: 1.8997 - val_acc: 0.7326\n",
      "Epoch 78/100\n",
      "40265/40265 [==============================] - 6s 161us/step - loss: 0.0439 - acc: 0.9855 - val_loss: 1.8969 - val_acc: 0.7332\n",
      "Epoch 79/100\n",
      "40265/40265 [==============================] - 6s 160us/step - loss: 0.0399 - acc: 0.9871 - val_loss: 1.9317 - val_acc: 0.7340\n",
      "Epoch 80/100\n",
      "40265/40265 [==============================] - 6s 153us/step - loss: 0.0453 - acc: 0.9847 - val_loss: 1.9465 - val_acc: 0.7296\n",
      "Epoch 81/100\n",
      "40265/40265 [==============================] - 6s 155us/step - loss: 0.0457 - acc: 0.9843 - val_loss: 1.9396 - val_acc: 0.7268\n",
      "Epoch 82/100\n",
      "40265/40265 [==============================] - 7s 180us/step - loss: 0.0450 - acc: 0.9841 - val_loss: 1.9347 - val_acc: 0.7317\n",
      "Epoch 83/100\n",
      "40265/40265 [==============================] - 7s 186us/step - loss: 0.0449 - acc: 0.9845 - val_loss: 1.9404 - val_acc: 0.7332\n",
      "Epoch 84/100\n",
      "40265/40265 [==============================] - 7s 167us/step - loss: 0.0402 - acc: 0.9866 - val_loss: 1.9351 - val_acc: 0.7345\n",
      "Epoch 85/100\n",
      "40265/40265 [==============================] - 6s 159us/step - loss: 0.0443 - acc: 0.9851 - val_loss: 1.9370 - val_acc: 0.7318\n",
      "Epoch 86/100\n",
      "40265/40265 [==============================] - 7s 165us/step - loss: 0.0481 - acc: 0.9832 - val_loss: 1.9416 - val_acc: 0.7285\n",
      "Epoch 87/100\n",
      "40265/40265 [==============================] - 7s 179us/step - loss: 0.0392 - acc: 0.9869 - val_loss: 1.9680 - val_acc: 0.7357\n",
      "Epoch 88/100\n",
      "40265/40265 [==============================] - 7s 178us/step - loss: 0.0401 - acc: 0.9859 - val_loss: 1.9639 - val_acc: 0.7327\n",
      "Epoch 89/100\n",
      "40265/40265 [==============================] - 10s 243us/step - loss: 0.0468 - acc: 0.9836 - val_loss: 1.9759 - val_acc: 0.72800.0468 - \n",
      "Epoch 90/100\n",
      "40265/40265 [==============================] - 9s 223us/step - loss: 0.0402 - acc: 0.9867 - val_loss: 1.9635 - val_acc: 0.7313\n",
      "Epoch 91/100\n",
      "40265/40265 [==============================] - 9s 223us/step - loss: 0.0441 - acc: 0.9846 - val_loss: 1.9723 - val_acc: 0.7329\n",
      "Epoch 92/100\n",
      "40265/40265 [==============================] - 10s 240us/step - loss: 0.0395 - acc: 0.9864 - val_loss: 1.9736 - val_acc: 0.7303\n",
      "Epoch 93/100\n",
      "40265/40265 [==============================] - 9s 235us/step - loss: 0.0446 - acc: 0.9839 - val_loss: 1.9732 - val_acc: 0.7314\n",
      "Epoch 94/100\n",
      "40265/40265 [==============================] - 8s 196us/step - loss: 0.0409 - acc: 0.9860 - val_loss: 1.9679 - val_acc: 0.7328\n",
      "Epoch 95/100\n",
      "40265/40265 [==============================] - 6s 149us/step - loss: 0.0439 - acc: 0.9848 - val_loss: 1.9689 - val_acc: 0.7351\n",
      "Epoch 96/100\n",
      "40265/40265 [==============================] - 6s 138us/step - loss: 0.0435 - acc: 0.9850 - val_loss: 1.9952 - val_acc: 0.7347\n",
      "Epoch 97/100\n",
      "40265/40265 [==============================] - 6s 141us/step - loss: 0.0415 - acc: 0.9865 - val_loss: 2.0072 - val_acc: 0.7285\n",
      "Epoch 98/100\n",
      "40265/40265 [==============================] - 6s 149us/step - loss: 0.0426 - acc: 0.9852 - val_loss: 1.9962 - val_acc: 0.7299\n",
      "Epoch 99/100\n",
      "40265/40265 [==============================] - 6s 158us/step - loss: 0.0397 - acc: 0.9864 - val_loss: 2.0029 - val_acc: 0.7336\n",
      "Epoch 100/100\n",
      "40265/40265 [==============================] - 6s 137us/step - loss: 0.0373 - acc: 0.9877 - val_loss: 1.9961 - val_acc: 0.7330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5890a160>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(70, input_dim=200, activation='relu'))\n",
    "model.add(Dense(25, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(xTrainS, yTrain, validation_data=(xTestS, yTest), epochs=100, verbose=1)\n",
    "# evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.38872969e-01]\n",
      " [8.56484912e-05]\n",
      " [6.81397432e-06]\n",
      " ...\n",
      " [8.75494406e-02]\n",
      " [1.04987135e-04]\n",
      " [1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict_proba(xTestS)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x3ab90908>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#predict=xgb.predict_proba(xTestS)\n",
    "\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = pred#[:, 1]\n",
    "\n",
    "# calculate AUC\n",
    "auc = roc_auc_score(yTest, probs)\n",
    "print('AUC: %.3f' % auc)\n",
    "# calculate roc curve\n",
    "fpr, tpr, thresholds = roc_curve(yTest, probs)\n",
    "# plot no skill\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot the roc curve for the model\n",
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.231242\n",
      "         Iterations 8\n",
      "                         Results: Logit\n",
      "=================================================================\n",
      "Model:              Logit            Pseudo R-squared: 0.291     \n",
      "Dependent Variable: target           AIC:              92854.9387\n",
      "Date:               2019-02-27 18:26 BIC:              94681.8257\n",
      "No. Observations:   200000           Log-Likelihood:   -46248.   \n",
      "Df Model:           178              LL-Null:          -65232.   \n",
      "Df Residuals:       199821           LLR p-value:      0.0000    \n",
      "Converged:          1.0000           Scale:            1.0000    \n",
      "No. Iterations:     8.0000                                       \n",
      "------------------------------------------------------------------\n",
      "              Coef.   Std.Err.     z      P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------\n",
      "var_0         0.0551    0.0028   19.6981  0.0000   0.0496   0.0606\n",
      "var_1         0.0406    0.0021   19.0574  0.0000   0.0364   0.0448\n",
      "var_2         0.0658    0.0032   20.4533  0.0000   0.0595   0.0721\n",
      "var_3         0.0175    0.0042    4.1412  0.0000   0.0092   0.0259\n",
      "var_4         0.0236    0.0053    4.4390  0.0000   0.0132   0.0340\n",
      "var_5         0.0138    0.0011   12.5567  0.0000   0.0116   0.0159\n",
      "var_6         0.2608    0.0099   26.4280  0.0000   0.2415   0.2802\n",
      "var_8         0.0182    0.0026    6.9868  0.0000   0.0131   0.0233\n",
      "var_9        -0.1110    0.0069  -15.9778  0.0000  -0.1247  -0.0974\n",
      "var_11        0.0124    0.0014    8.5728  0.0000   0.0096   0.0152\n",
      "var_12       -1.1153    0.0446  -25.0324  0.0000  -1.2026  -1.0280\n",
      "var_13       -0.0389    0.0018  -21.0688  0.0000  -0.0425  -0.0353\n",
      "var_15        0.1430    0.0210    6.8209  0.0000   0.1019   0.1841\n",
      "var_16        0.0083    0.0034    2.4623  0.0138   0.0017   0.0149\n",
      "var_18        0.0171    0.0011   15.6548  0.0000   0.0150   0.0193\n",
      "var_19        0.0044    0.0011    4.0398  0.0001   0.0022   0.0065\n",
      "var_20       -0.0107    0.0015   -7.2889  0.0000  -0.0136  -0.0078\n",
      "var_21       -0.0238    0.0011  -22.5301  0.0000  -0.0258  -0.0217\n",
      "var_22        0.0706    0.0030   23.5669  0.0000   0.0647   0.0765\n",
      "var_23       -0.1719    0.0164  -10.4558  0.0000  -0.2041  -0.1397\n",
      "var_24        0.0263    0.0023   11.5314  0.0000   0.0218   0.0308\n",
      "var_25        0.1564    0.0301    5.1933  0.0000   0.0974   0.2154\n",
      "var_26        0.0334    0.0014   23.2874  0.0000   0.0306   0.0362\n",
      "var_28       -0.1027    0.0110   -9.3067  0.0000  -0.1243  -0.0811\n",
      "var_29        0.0087    0.0033    2.6439  0.0082   0.0023   0.0152\n",
      "var_31       -0.0407    0.0040  -10.1165  0.0000  -0.0485  -0.0328\n",
      "var_32        0.0392    0.0033   11.7516  0.0000   0.0327   0.0458\n",
      "var_33       -0.0356    0.0020  -17.7047  0.0000  -0.0395  -0.0317\n",
      "var_34       -0.3222    0.0159  -20.2086  0.0000  -0.3535  -0.2910\n",
      "var_35        0.0236    0.0017   14.1611  0.0000   0.0203   0.0269\n",
      "var_36       -0.0410    0.0028  -14.7926  0.0000  -0.0464  -0.0355\n",
      "var_37        0.0117    0.0038    3.0505  0.0023   0.0042   0.0192\n",
      "var_39       -0.0031    0.0021   -1.4565  0.1453  -0.0073   0.0011\n",
      "var_40        0.0202    0.0010   19.4896  0.0000   0.0182   0.0223\n",
      "var_42       -0.0403    0.0124   -3.2429  0.0012  -0.0646  -0.0159\n",
      "var_43       -0.2841    0.0279  -10.2008  0.0000  -0.3387  -0.2295\n",
      "var_44       -0.0281    0.0014  -19.6279  0.0000  -0.0309  -0.0253\n",
      "var_45       -0.0032    0.0004   -7.9533  0.0000  -0.0040  -0.0024\n",
      "var_46        0.0059    0.0030    1.9647  0.0494   0.0000   0.0118\n",
      "var_47        0.0031    0.0008    3.8223  0.0001   0.0015   0.0047\n",
      "var_48        0.0093    0.0008   12.2375  0.0000   0.0078   0.0108\n",
      "var_49        0.0121    0.0011   11.0002  0.0000   0.0099   0.0142\n",
      "var_50       -0.0634    0.0125   -5.0900  0.0000  -0.0879  -0.0390\n",
      "var_51        0.0091    0.0011    8.6300  0.0000   0.0070   0.0111\n",
      "var_52        0.0183    0.0017   10.5274  0.0000   0.0149   0.0216\n",
      "var_53        0.2801    0.0112   24.9384  0.0000   0.2581   0.3021\n",
      "var_54       -0.0068    0.0010   -6.5709  0.0000  -0.0088  -0.0047\n",
      "var_55        0.0108    0.0015    7.1146  0.0000   0.0078   0.0138\n",
      "var_56       -0.0315    0.0024  -13.0174  0.0000  -0.0363  -0.0268\n",
      "var_57       -0.0671    0.0109   -6.1648  0.0000  -0.0884  -0.0457\n",
      "var_58       -0.0192    0.0020   -9.5746  0.0000  -0.0232  -0.0153\n",
      "var_59       -0.0425    0.0101   -4.1995  0.0000  -0.0624  -0.0227\n",
      "var_60        0.0056    0.0020    2.7502  0.0060   0.0016   0.0096\n",
      "var_61        0.0025    0.0007    3.3653  0.0008   0.0010   0.0040\n",
      "var_62        0.0268    0.0042    6.3207  0.0000   0.0185   0.0351\n",
      "var_63       -0.0152    0.0028   -5.4849  0.0000  -0.0206  -0.0098\n",
      "var_64       -0.0285    0.0058   -4.9106  0.0000  -0.0399  -0.0171\n",
      "var_65        0.0079    0.0023    3.4571  0.0005   0.0034   0.0124\n",
      "var_66        0.0642    0.0077    8.3533  0.0000   0.0491   0.0792\n",
      "var_67        0.0202    0.0012   17.1729  0.0000   0.0179   0.0225\n",
      "var_68        6.1893    0.2692   22.9958  0.0000   5.6618   6.7168\n",
      "var_69        0.0053    0.0022    2.4407  0.0147   0.0010   0.0096\n",
      "var_70        0.0075    0.0007   10.3896  0.0000   0.0061   0.0090\n",
      "var_71        0.4182    0.0324   12.9125  0.0000   0.3547   0.4817\n",
      "var_72       -0.0111    0.0022   -5.0475  0.0000  -0.0154  -0.0068\n",
      "var_73       -0.0029    0.0012   -2.5056  0.0122  -0.0052  -0.0006\n",
      "var_74        0.0047    0.0006    7.5644  0.0000   0.0034   0.0059\n",
      "var_75       -0.0204    0.0014  -14.3563  0.0000  -0.0231  -0.0176\n",
      "var_76       -0.0262    0.0011  -24.3027  0.0000  -0.0283  -0.0241\n",
      "var_77       -0.0151    0.0023   -6.6220  0.0000  -0.0195  -0.0106\n",
      "var_78        0.0822    0.0043   18.9216  0.0000   0.0737   0.0907\n",
      "var_79        0.0165    0.0066    2.4945  0.0126   0.0035   0.0294\n",
      "var_80       -0.0250    0.0011  -21.8733  0.0000  -0.0272  -0.0227\n",
      "var_81       -0.1114    0.0037  -30.3579  0.0000  -0.1185  -0.1042\n",
      "var_82        0.0085    0.0010    8.3415  0.0000   0.0065   0.0105\n",
      "var_83       -0.0082    0.0010   -7.9279  0.0000  -0.0103  -0.0062\n",
      "var_84        0.0056    0.0014    4.0401  0.0001   0.0029   0.0083\n",
      "var_85       -0.0181    0.0022   -8.1598  0.0000  -0.0224  -0.0138\n",
      "var_86       -0.0166    0.0011  -15.1059  0.0000  -0.0188  -0.0145\n",
      "var_87       -0.0210    0.0015  -13.7097  0.0000  -0.0240  -0.0180\n",
      "var_88       -0.0247    0.0035   -7.1392  0.0000  -0.0315  -0.0180\n",
      "var_89        0.0374    0.0024   15.5125  0.0000   0.0327   0.0421\n",
      "var_90        0.0072    0.0007   10.9022  0.0000   0.0059   0.0085\n",
      "var_91        0.8617    0.0563   15.2985  0.0000   0.7513   0.9721\n",
      "var_92       -0.0352    0.0021  -17.0235  0.0000  -0.0392  -0.0311\n",
      "var_93       -0.1929    0.0157  -12.2779  0.0000  -0.2237  -0.1621\n",
      "var_94        0.0585    0.0031   18.8481  0.0000   0.0524   0.0645\n",
      "var_95        0.1996    0.0138   14.4729  0.0000   0.1726   0.2266\n",
      "var_97        0.0039    0.0007    5.7764  0.0000   0.0026   0.0053\n",
      "var_99        0.1059    0.0046   23.0331  0.0000   0.0969   0.1150\n",
      "var_101      -0.0063    0.0017   -3.5996  0.0003  -0.0097  -0.0029\n",
      "var_102      -0.0071    0.0010   -7.1331  0.0000  -0.0091  -0.0052\n",
      "var_104      -0.0466    0.0044  -10.5900  0.0000  -0.0553  -0.0380\n",
      "var_105       0.0959    0.0101    9.5197  0.0000   0.0761   0.1156\n",
      "var_106       0.0585    0.0045   12.8551  0.0000   0.0496   0.0674\n",
      "var_107      -0.0183    0.0011  -16.0539  0.0000  -0.0206  -0.0161\n",
      "var_108      -0.7958    0.0497  -16.0150  0.0000  -0.8932  -0.6984\n",
      "var_109      -0.0360    0.0020  -18.2037  0.0000  -0.0399  -0.0321\n",
      "var_110       0.0534    0.0022   23.9300  0.0000   0.0491   0.0578\n",
      "var_111       0.0781    0.0079    9.8416  0.0000   0.0626   0.0937\n",
      "var_112       0.0499    0.0055    9.1226  0.0000   0.0391   0.0606\n",
      "var_113      -0.0108    0.0019   -5.5970  0.0000  -0.0146  -0.0070\n",
      "var_114      -0.0838    0.0087   -9.5910  0.0000  -0.1009  -0.0667\n",
      "var_115      -0.0611    0.0033  -18.6481  0.0000  -0.0675  -0.0547\n",
      "var_116      -0.0491    0.0052   -9.3958  0.0000  -0.0593  -0.0389\n",
      "var_118       0.0154    0.0010   15.6374  0.0000   0.0135   0.0173\n",
      "var_119       0.0226    0.0021   10.9983  0.0000   0.0186   0.0266\n",
      "var_120      -0.0026    0.0007   -3.6184  0.0003  -0.0040  -0.0012\n",
      "var_121      -0.0767    0.0051  -15.1275  0.0000  -0.0866  -0.0668\n",
      "var_122      -0.0280    0.0017  -16.8002  0.0000  -0.0313  -0.0247\n",
      "var_123      -0.0209    0.0014  -15.0105  0.0000  -0.0236  -0.0181\n",
      "var_125       0.3122    0.0270   11.5800  0.0000   0.2594   0.3651\n",
      "var_127      -0.0408    0.0027  -14.8798  0.0000  -0.0462  -0.0354\n",
      "var_128       0.0278    0.0027   10.4249  0.0000   0.0226   0.0331\n",
      "var_129      -0.0049    0.0021   -2.3632  0.0181  -0.0090  -0.0008\n",
      "var_130       0.1270    0.0103   12.2867  0.0000   0.1068   0.1473\n",
      "var_131      -0.2113    0.0189  -11.1992  0.0000  -0.2483  -0.1744\n",
      "var_132      -0.0576    0.0059   -9.7197  0.0000  -0.0692  -0.0460\n",
      "var_133       0.4660    0.0227   20.5162  0.0000   0.4215   0.5106\n",
      "var_134       0.0096    0.0014    6.8514  0.0000   0.0068   0.0123\n",
      "var_135       0.0111    0.0011    9.8360  0.0000   0.0089   0.0133\n",
      "var_137       0.0106    0.0010   10.8628  0.0000   0.0087   0.0125\n",
      "var_138       0.0134    0.0019    7.0278  0.0000   0.0096   0.0171\n",
      "var_139      -0.0308    0.0011  -27.6528  0.0000  -0.0329  -0.0286\n",
      "var_140       0.0123    0.0018    6.9292  0.0000   0.0088   0.0157\n",
      "var_141      -0.0147    0.0013  -11.4728  0.0000  -0.0172  -0.0122\n",
      "var_142      -0.0112    0.0015   -7.3918  0.0000  -0.0142  -0.0082\n",
      "var_143      -0.0150    0.0029   -5.0871  0.0000  -0.0207  -0.0092\n",
      "var_144       0.0779    0.0094    8.3321  0.0000   0.0596   0.0963\n",
      "var_145       0.0254    0.0022   11.4772  0.0000   0.0211   0.0297\n",
      "var_146      -0.0816    0.0034  -24.1296  0.0000  -0.0882  -0.0749\n",
      "var_147       0.0172    0.0012   14.8314  0.0000   0.0150   0.0195\n",
      "var_148      -0.8482    0.0429  -19.7659  0.0000  -0.9324  -0.7641\n",
      "var_149      -0.0144    0.0008  -17.3487  0.0000  -0.0161  -0.0128\n",
      "var_150      -0.0374    0.0035  -10.6680  0.0000  -0.0443  -0.0305\n",
      "var_151       0.0235    0.0022   10.8746  0.0000   0.0193   0.0278\n",
      "var_152      -0.0115    0.0029   -3.9938  0.0001  -0.0171  -0.0058\n",
      "var_154      -0.0296    0.0017  -17.0866  0.0000  -0.0330  -0.0262\n",
      "var_155       0.0216    0.0015   14.4843  0.0000   0.0186   0.0245\n",
      "var_156      -0.0671    0.0091   -7.4179  0.0000  -0.0849  -0.0494\n",
      "var_157       0.0184    0.0015   11.9699  0.0000   0.0154   0.0214\n",
      "var_159       0.0127    0.0021    6.0375  0.0000   0.0086   0.0168\n",
      "var_161       0.0821    0.0396    2.0719  0.0383   0.0044   0.1598\n",
      "var_162       0.0755    0.0061   12.4373  0.0000   0.0636   0.0874\n",
      "var_163       0.0194    0.0016   11.9385  0.0000   0.0162   0.0226\n",
      "var_164       0.0250    0.0016   15.7401  0.0000   0.0219   0.0281\n",
      "var_165      -0.0351    0.0017  -20.4685  0.0000  -0.0385  -0.0317\n",
      "var_166      -0.4912    0.0232  -21.1418  0.0000  -0.5368  -0.4457\n",
      "var_167       0.0125    0.0011   11.2897  0.0000   0.0103   0.0147\n",
      "var_168       0.0146    0.0028    5.2933  0.0000   0.0092   0.0200\n",
      "var_169      -0.4155    0.0234  -17.7510  0.0000  -0.4614  -0.3697\n",
      "var_170       0.0364    0.0019   18.8149  0.0000   0.0326   0.0402\n",
      "var_171       0.0084    0.0016    5.2634  0.0000   0.0053   0.0116\n",
      "var_172      -0.0145    0.0010  -14.5513  0.0000  -0.0164  -0.0125\n",
      "var_173       0.0241    0.0015   16.6018  0.0000   0.0213   0.0270\n",
      "var_174      -0.0280    0.0012  -23.3047  0.0000  -0.0303  -0.0256\n",
      "var_175       0.0275    0.0030    9.2460  0.0000   0.0217   0.0333\n",
      "var_176       0.0042    0.0012    3.6560  0.0003   0.0020   0.0065\n",
      "var_177      -0.0482    0.0033  -14.6300  0.0000  -0.0547  -0.0418\n",
      "var_178      -0.0070    0.0010   -6.9964  0.0000  -0.0090  -0.0051\n",
      "var_179       0.0590    0.0030   19.5130  0.0000   0.0530   0.0649\n",
      "var_180       0.0197    0.0016   12.0550  0.0000   0.0165   0.0229\n",
      "var_181       0.0371    0.0063    5.9104  0.0000   0.0248   0.0495\n",
      "var_182      -0.0038    0.0010   -3.9095  0.0001  -0.0057  -0.0019\n",
      "var_184       0.0175    0.0009   18.9718  0.0000   0.0157   0.0193\n",
      "var_186      -0.0310    0.0027  -11.4251  0.0000  -0.0363  -0.0257\n",
      "var_187       0.0044    0.0007    5.8149  0.0000   0.0029   0.0058\n",
      "var_188      -0.0293    0.0022  -13.4200  0.0000  -0.0336  -0.0251\n",
      "var_189       0.0255    0.0089    2.8785  0.0040   0.0081   0.0429\n",
      "var_190       0.0409    0.0019   21.6154  0.0000   0.0372   0.0446\n",
      "var_191       0.0515    0.0028   18.2881  0.0000   0.0460   0.0571\n",
      "var_192      -0.0971    0.0059  -16.5203  0.0000  -0.1087  -0.0856\n",
      "var_193      -0.0152    0.0022   -7.0342  0.0000  -0.0195  -0.0110\n",
      "var_194      -0.0229    0.0028   -8.3281  0.0000  -0.0283  -0.0175\n",
      "var_195       0.0673    0.0060   11.1948  0.0000   0.0555   0.0791\n",
      "var_196       0.0136    0.0016    8.5913  0.0000   0.0105   0.0167\n",
      "var_197      -0.1310    0.0094  -14.0006  0.0000  -0.1494  -0.1127\n",
      "var_198      -0.0573    0.0028  -20.2340  0.0000  -0.0628  -0.0517\n",
      "var_199       0.0079    0.0008    9.4400  0.0000   0.0062   0.0095\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier as XGB\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain,xTest,yTrain,yTest=tts(xr,yr,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of oversampled data is  241202\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"data_final=data.columns.values.tolist()\\ny=['target']\\nX=[i for i in data_final_vars if i not in y]\\n\\nfrom sklearn.feature_selection import RFE\\nfrom sklearn.linear_model import LogisticRegression\\n\\nlogreg = LogisticRegression()\\nrfe = RFE(logreg, 20)\\n\\nrfe = rfe.fit(osX,osY.values.ravel())\\nprint(rfe.support_)\\nprint(rfe.ranking_)\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#important features extraction\n",
    "'''data_final=data.columns.values.tolist()\n",
    "y=['target']\n",
    "X=[i for i in data_final_vars if i not in y]\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "rfe = RFE(logreg, 20)\n",
    "\n",
    "rfe = rfe.fit(osX,osY.values.ravel())\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn import decomposition\\npca = decomposition.PCA()\\nfa = decomposition.FactorAnalysis()\\n#X = df.values[:, 0:4]\\n#Y = df.values[:, 4]\\n#train, test = train_test_split(X,test_size = 0.3)\\nxTrain,xTest,yTrain,yTest=tts(xr,yr,test_size=0.33,random_state=42)\\nxTrain = pca.fit_transform(xTrain)\\nxTest = pca.transform(xTest)\\npca.n_components_\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this may help us to know the most important features\n",
    "'''from sklearn import decomposition\n",
    "pca = decomposition.PCA()\n",
    "fa = decomposition.FactorAnalysis()\n",
    "#X = df.values[:, 0:4]\n",
    "#Y = df.values[:, 4]\n",
    "#train, test = train_test_split(X,test_size = 0.3)\n",
    "xTrain,xTest,yTrain,yTest=tts(xr,yr,test_size=0.33,random_state=42)\n",
    "xTrain = pca.fit_transform(xTrain)\n",
    "xTest = pca.transform(xTest)\n",
    "pca.n_components_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler as ss\n",
    "scalar=ss()\n",
    "xTrainS=scalar.fit(xTrain).fit_transform(xTrain)\n",
    "xTestS=scalar.fit(xTest).fit_transform(xTest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\program files\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=42, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          learning_rate=0.1, n_estimators=100, random_state=42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "model=AdaBoostClassifier(n_estimators=100, base_estimator=lr(penalty='l2',random_state=42),learning_rate=0.1,random_state=42)\n",
    "model.fit(xTrainS,yTrain)\n",
    "\n",
    "\n",
    "#from sklearn.ensemble import GradientBoostingClassifier\n",
    "#for regression\n",
    "#from sklearn.ensemble import GradientBoostingRegressor\n",
    "#model = XGB()\n",
    "#model.fit(xTrainS, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featimp = pd.Series(model.feature_importances_,index=xTrain.columns).sort_values(ascending=False)\n",
    "#print(featimp) # this is the property of Random Forest classifier t\n",
    "#hat it provide us the importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer as tft\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.model_selection import KFold as kf\n",
    "from sklearn.model_selection import cross_val_score as cvs\n",
    "from sklearn.feature_extraction.text import CountVectorizer as cv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tv\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import RobustScaler as rs\n",
    "from sklearn.metrics import classification_report as cr\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
    "from sklearn.kernel_ridge import KernelRidge as kr\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from sklearn.metrics import f1_score as fs\n",
    "from sklearn.linear_model import LinearRegression as Lr\n",
    "from sklearn.pipeline import make_pipeline as mp\n",
    "from sklearn.naive_bayes import MultinomialNB as mnb\n",
    "from sklearn.base import BaseEstimator as be\n",
    "from sklearn.base import TransformerMixin as tm\n",
    "from sklearn.base import RegressorMixin as rm\n",
    "from sklearn.base import clone\n",
    "from nltk.stem import PorterStemmer as ps\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier as knc\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB as gn\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as ld\n",
    "from sklearn.tree import DecisionTreeClassifier as dtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3939b7dfca5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#for name,model in models:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcv_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxTrainS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myTrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#results.append(cv_result)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#names.append(name)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    823\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    698\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1045\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1046\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#models=[]\n",
    "#models.append(('XGB',XGB().fit(xTrainS,yTrain)))\n",
    "\n",
    "score='accuracy'\n",
    "#names=[]\n",
    "#results=[]\n",
    "#for name,model in models:\n",
    "kfold=ms.KFold(n_splits=100,random_state=42)\n",
    "cv_result=ms.cross_val_score(model,xTrainS,yTrain,cv=kfold,scoring='accuracy')\n",
    "#results.append(cv_result)\n",
    "#names.append(name)\n",
    "prediction=model.predict(xTestS)\n",
    "prediction=[round(value) for value in predict]\n",
    "    \n",
    "msg='XGB:  Accuracy: %f  std(%f)'%(cv_result.mean()*100,cv_result.std())\n",
    "    \n",
    "con=cm(yTest,prediction)\n",
    "print(msg,'Confusion-Matrix:',con)\n",
    "print('Classification Report')\n",
    "print(cr(yTest,prediction))\n",
    "print(\"f1_score\",fs(yTest,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy-score: 0.7973908502897183\n",
      "Confusion-Matrix\n",
      "[[46925 12452]\n",
      " [11605 47754]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80     59377\n",
      "           1       0.79      0.80      0.80     59359\n",
      "\n",
      "   micro avg       0.80      0.80      0.80    118736\n",
      "   macro avg       0.80      0.80      0.80    118736\n",
      "weighted avg       0.80      0.80      0.80    118736\n",
      "\n",
      "f1_score 0.7987956341738802\n"
     ]
    }
   ],
   "source": [
    "predict=model.predict(xTestS)\n",
    "predict=[round(value) for value in predict]\n",
    "print('Accuracy-score:',acc(yTest,predict))\n",
    "print('Confusion-Matrix')\n",
    "print(cm(yTest,predict))\n",
    "print('Classification Report')\n",
    "print(cr(yTest,predict))\n",
    "print(\"f1_score\",fs(yTest,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('test.csv')\n",
    "\n",
    "test.drop('ID_code',axis=1,inplace=True)\n",
    "test.drop(col,inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainS=scalar.fit(xr).fit_transform(xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testS=scalar.fit(test).fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainS,yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(testS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions=[round(value) for value in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=pd.read_csv('test.csv')\n",
    "\n",
    "final=pd.DataFrame({'ID_code':test2['ID_code'],'target':predictions})\n",
    "final.to_csv('customresult2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94671, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3=pd.read_csv('customresult2.csv')\n",
    "t=test3[test3.target ==1]\n",
    "t.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 98767   96134   6118   70038   95328     96282  94686   \n",
    "#0 101233  103866  193882 129962  104672   103718  105314 105329"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031555275889458535"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6118/193882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "param = {\n",
    "'num_leaves': 3,\n",
    "'max_bin': 63,\n",
    "'min_data_in_leaf': 5,\n",
    "'lambda_l1': 2,\n",
    "'lambda_l2': 5,\n",
    "'min_gain_to_split': 0.29588543202055562,\n",
    "'max_depth': 2,\n",
    "#'save_binary': True,\n",
    "'seed': 1337,\n",
    "'feature_fraction_seed': 1337,\n",
    "'bagging_seed': 1337,\n",
    "'drop_seed': 1337,\n",
    "'data_random_seed': 1337,\n",
    "'objective': 'binary',\n",
    "'boosting_type': 'dart','xgboost_dart_mode':True,\n",
    "'verbose': 1,\n",
    "'metric': 'auc',\n",
    "#'is_unbalance': True,\n",
    "#'boost_from_average': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=lgb.Dataset(xTrainS,label=yTrain)\n",
    "valid_data=lgb.Dataset(xTestS,label=yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\tvalid_0's auc: 0.60743\n",
      "[10]\tvalid_0's auc: 0.662023\n",
      "[15]\tvalid_0's auc: 0.685231\n",
      "[20]\tvalid_0's auc: 0.686983\n",
      "[25]\tvalid_0's auc: 0.700291\n",
      "[30]\tvalid_0's auc: 0.706729\n",
      "[35]\tvalid_0's auc: 0.718444\n",
      "[40]\tvalid_0's auc: 0.723962\n",
      "[45]\tvalid_0's auc: 0.727561\n",
      "[50]\tvalid_0's auc: 0.742344\n",
      "[55]\tvalid_0's auc: 0.747527\n",
      "[60]\tvalid_0's auc: 0.754816\n",
      "[65]\tvalid_0's auc: 0.758917\n",
      "[70]\tvalid_0's auc: 0.761416\n",
      "[75]\tvalid_0's auc: 0.767972\n",
      "[80]\tvalid_0's auc: 0.768003\n",
      "[85]\tvalid_0's auc: 0.76732\n",
      "[90]\tvalid_0's auc: 0.767959\n",
      "[95]\tvalid_0's auc: 0.771528\n",
      "[100]\tvalid_0's auc: 0.774348\n",
      "[105]\tvalid_0's auc: 0.777457\n",
      "[110]\tvalid_0's auc: 0.778797\n",
      "[115]\tvalid_0's auc: 0.784301\n",
      "[120]\tvalid_0's auc: 0.786075\n",
      "[125]\tvalid_0's auc: 0.787569\n",
      "[130]\tvalid_0's auc: 0.787707\n",
      "[135]\tvalid_0's auc: 0.788346\n",
      "[140]\tvalid_0's auc: 0.79139\n",
      "[145]\tvalid_0's auc: 0.793028\n",
      "[150]\tvalid_0's auc: 0.795004\n",
      "[155]\tvalid_0's auc: 0.797292\n",
      "[160]\tvalid_0's auc: 0.797888\n",
      "[165]\tvalid_0's auc: 0.799418\n",
      "[170]\tvalid_0's auc: 0.800151\n",
      "[175]\tvalid_0's auc: 0.802388\n",
      "[180]\tvalid_0's auc: 0.804392\n",
      "[185]\tvalid_0's auc: 0.804597\n",
      "[190]\tvalid_0's auc: 0.808299\n",
      "[195]\tvalid_0's auc: 0.81002\n",
      "[200]\tvalid_0's auc: 0.810545\n",
      "[205]\tvalid_0's auc: 0.810607\n",
      "[210]\tvalid_0's auc: 0.81052\n",
      "[215]\tvalid_0's auc: 0.812222\n",
      "[220]\tvalid_0's auc: 0.813714\n",
      "[225]\tvalid_0's auc: 0.814271\n",
      "[230]\tvalid_0's auc: 0.816652\n",
      "[235]\tvalid_0's auc: 0.816832\n",
      "[240]\tvalid_0's auc: 0.818752\n",
      "[245]\tvalid_0's auc: 0.820077\n",
      "[250]\tvalid_0's auc: 0.821716\n",
      "[255]\tvalid_0's auc: 0.822828\n",
      "[260]\tvalid_0's auc: 0.822772\n",
      "[265]\tvalid_0's auc: 0.824916\n",
      "[270]\tvalid_0's auc: 0.825851\n",
      "[275]\tvalid_0's auc: 0.826227\n",
      "[280]\tvalid_0's auc: 0.826839\n",
      "[285]\tvalid_0's auc: 0.827037\n",
      "[290]\tvalid_0's auc: 0.828532\n",
      "[295]\tvalid_0's auc: 0.829269\n",
      "[300]\tvalid_0's auc: 0.830248\n",
      "[305]\tvalid_0's auc: 0.830689\n",
      "[310]\tvalid_0's auc: 0.831929\n",
      "[315]\tvalid_0's auc: 0.832281\n",
      "[320]\tvalid_0's auc: 0.832215\n",
      "[325]\tvalid_0's auc: 0.832461\n",
      "[330]\tvalid_0's auc: 0.833574\n",
      "[335]\tvalid_0's auc: 0.834572\n",
      "[340]\tvalid_0's auc: 0.835303\n",
      "[345]\tvalid_0's auc: 0.83539\n",
      "[350]\tvalid_0's auc: 0.836465\n",
      "[355]\tvalid_0's auc: 0.836741\n",
      "[360]\tvalid_0's auc: 0.837343\n",
      "[365]\tvalid_0's auc: 0.838026\n",
      "[370]\tvalid_0's auc: 0.83855\n",
      "[375]\tvalid_0's auc: 0.839308\n",
      "[380]\tvalid_0's auc: 0.840228\n",
      "[385]\tvalid_0's auc: 0.840773\n",
      "[390]\tvalid_0's auc: 0.841429\n",
      "[395]\tvalid_0's auc: 0.842234\n",
      "[400]\tvalid_0's auc: 0.842236\n",
      "[405]\tvalid_0's auc: 0.842546\n",
      "[410]\tvalid_0's auc: 0.842576\n",
      "[415]\tvalid_0's auc: 0.8427\n",
      "[420]\tvalid_0's auc: 0.843047\n",
      "[425]\tvalid_0's auc: 0.84347\n",
      "[430]\tvalid_0's auc: 0.843181\n",
      "[435]\tvalid_0's auc: 0.843554\n",
      "[440]\tvalid_0's auc: 0.843673\n",
      "[445]\tvalid_0's auc: 0.843652\n",
      "[450]\tvalid_0's auc: 0.844421\n",
      "[455]\tvalid_0's auc: 0.844719\n",
      "[460]\tvalid_0's auc: 0.84513\n",
      "[465]\tvalid_0's auc: 0.845479\n",
      "[470]\tvalid_0's auc: 0.84553\n",
      "[475]\tvalid_0's auc: 0.846206\n",
      "[480]\tvalid_0's auc: 0.846598\n",
      "[485]\tvalid_0's auc: 0.846975\n",
      "[490]\tvalid_0's auc: 0.847362\n",
      "[495]\tvalid_0's auc: 0.847945\n",
      "[500]\tvalid_0's auc: 0.848289\n",
      "[505]\tvalid_0's auc: 0.848496\n",
      "[510]\tvalid_0's auc: 0.848919\n",
      "[515]\tvalid_0's auc: 0.849276\n",
      "[520]\tvalid_0's auc: 0.84978\n",
      "[525]\tvalid_0's auc: 0.850233\n",
      "[530]\tvalid_0's auc: 0.850468\n",
      "[535]\tvalid_0's auc: 0.850727\n",
      "[540]\tvalid_0's auc: 0.851183\n",
      "[545]\tvalid_0's auc: 0.851575\n",
      "[550]\tvalid_0's auc: 0.851832\n",
      "[555]\tvalid_0's auc: 0.852121\n",
      "[560]\tvalid_0's auc: 0.852355\n",
      "[565]\tvalid_0's auc: 0.852829\n",
      "[570]\tvalid_0's auc: 0.852687\n",
      "[575]\tvalid_0's auc: 0.852695\n",
      "[580]\tvalid_0's auc: 0.85303\n",
      "[585]\tvalid_0's auc: 0.853411\n",
      "[590]\tvalid_0's auc: 0.853567\n",
      "[595]\tvalid_0's auc: 0.853583\n",
      "[600]\tvalid_0's auc: 0.853603\n",
      "[605]\tvalid_0's auc: 0.854079\n",
      "[610]\tvalid_0's auc: 0.854377\n",
      "[615]\tvalid_0's auc: 0.854812\n",
      "[620]\tvalid_0's auc: 0.85494\n",
      "[625]\tvalid_0's auc: 0.855422\n",
      "[630]\tvalid_0's auc: 0.855993\n",
      "[635]\tvalid_0's auc: 0.856122\n",
      "[640]\tvalid_0's auc: 0.856161\n",
      "[645]\tvalid_0's auc: 0.856345\n",
      "[650]\tvalid_0's auc: 0.856736\n",
      "[655]\tvalid_0's auc: 0.857\n",
      "[660]\tvalid_0's auc: 0.857592\n",
      "[665]\tvalid_0's auc: 0.85792\n",
      "[670]\tvalid_0's auc: 0.858168\n",
      "[675]\tvalid_0's auc: 0.858684\n",
      "[680]\tvalid_0's auc: 0.858854\n",
      "[685]\tvalid_0's auc: 0.859002\n",
      "[690]\tvalid_0's auc: 0.85943\n",
      "[695]\tvalid_0's auc: 0.859636\n",
      "[700]\tvalid_0's auc: 0.860256\n",
      "[705]\tvalid_0's auc: 0.860558\n",
      "[710]\tvalid_0's auc: 0.860548\n",
      "[715]\tvalid_0's auc: 0.860866\n",
      "[720]\tvalid_0's auc: 0.860946\n",
      "[725]\tvalid_0's auc: 0.861284\n",
      "[730]\tvalid_0's auc: 0.861433\n",
      "[735]\tvalid_0's auc: 0.861811\n",
      "[740]\tvalid_0's auc: 0.862097\n",
      "[745]\tvalid_0's auc: 0.862405\n",
      "[750]\tvalid_0's auc: 0.862357\n",
      "[755]\tvalid_0's auc: 0.862403\n",
      "[760]\tvalid_0's auc: 0.862546\n",
      "[765]\tvalid_0's auc: 0.862526\n",
      "[770]\tvalid_0's auc: 0.862644\n",
      "[775]\tvalid_0's auc: 0.862782\n",
      "[780]\tvalid_0's auc: 0.863071\n",
      "[785]\tvalid_0's auc: 0.863169\n",
      "[790]\tvalid_0's auc: 0.863834\n",
      "[795]\tvalid_0's auc: 0.863944\n",
      "[800]\tvalid_0's auc: 0.86453\n",
      "[805]\tvalid_0's auc: 0.864892\n",
      "[810]\tvalid_0's auc: 0.864984\n",
      "[815]\tvalid_0's auc: 0.864981\n",
      "[820]\tvalid_0's auc: 0.865055\n",
      "[825]\tvalid_0's auc: 0.865254\n",
      "[830]\tvalid_0's auc: 0.865279\n",
      "[835]\tvalid_0's auc: 0.86538\n",
      "[840]\tvalid_0's auc: 0.865368\n",
      "[845]\tvalid_0's auc: 0.865731\n",
      "[850]\tvalid_0's auc: 0.866263\n",
      "[855]\tvalid_0's auc: 0.86638\n",
      "[860]\tvalid_0's auc: 0.86649\n",
      "[865]\tvalid_0's auc: 0.866651\n",
      "[870]\tvalid_0's auc: 0.866931\n",
      "[875]\tvalid_0's auc: 0.867008\n",
      "[880]\tvalid_0's auc: 0.867289\n",
      "[885]\tvalid_0's auc: 0.867382\n",
      "[890]\tvalid_0's auc: 0.86761\n",
      "[895]\tvalid_0's auc: 0.867835\n",
      "[900]\tvalid_0's auc: 0.868007\n",
      "[905]\tvalid_0's auc: 0.868153\n",
      "[910]\tvalid_0's auc: 0.8683\n",
      "[915]\tvalid_0's auc: 0.868259\n",
      "[920]\tvalid_0's auc: 0.868436\n",
      "[925]\tvalid_0's auc: 0.868453\n",
      "[930]\tvalid_0's auc: 0.868785\n",
      "[935]\tvalid_0's auc: 0.869221\n",
      "[940]\tvalid_0's auc: 0.869477\n",
      "[945]\tvalid_0's auc: 0.869535\n",
      "[950]\tvalid_0's auc: 0.869655\n",
      "[955]\tvalid_0's auc: 0.869706\n",
      "[960]\tvalid_0's auc: 0.869815\n",
      "[965]\tvalid_0's auc: 0.869936\n",
      "[970]\tvalid_0's auc: 0.870309\n",
      "[975]\tvalid_0's auc: 0.870484\n",
      "[980]\tvalid_0's auc: 0.870564\n",
      "[985]\tvalid_0's auc: 0.870729\n",
      "[990]\tvalid_0's auc: 0.870839\n",
      "[995]\tvalid_0's auc: 0.870958\n",
      "[1000]\tvalid_0's auc: 0.871063\n",
      "[1005]\tvalid_0's auc: 0.871084\n",
      "[1010]\tvalid_0's auc: 0.871117\n",
      "[1015]\tvalid_0's auc: 0.871284\n",
      "[1020]\tvalid_0's auc: 0.871318\n",
      "[1025]\tvalid_0's auc: 0.871491\n",
      "[1030]\tvalid_0's auc: 0.871622\n",
      "[1035]\tvalid_0's auc: 0.871705\n",
      "[1040]\tvalid_0's auc: 0.871854\n",
      "[1045]\tvalid_0's auc: 0.871944\n",
      "[1050]\tvalid_0's auc: 0.872076\n",
      "[1055]\tvalid_0's auc: 0.872099\n",
      "[1060]\tvalid_0's auc: 0.872205\n",
      "[1065]\tvalid_0's auc: 0.872181\n",
      "[1070]\tvalid_0's auc: 0.872232\n",
      "[1075]\tvalid_0's auc: 0.872419\n",
      "[1080]\tvalid_0's auc: 0.872557\n",
      "[1085]\tvalid_0's auc: 0.872597\n",
      "[1090]\tvalid_0's auc: 0.872819\n",
      "[1095]\tvalid_0's auc: 0.872922\n",
      "[1100]\tvalid_0's auc: 0.873037\n",
      "[1105]\tvalid_0's auc: 0.873135\n",
      "[1110]\tvalid_0's auc: 0.873154\n",
      "[1115]\tvalid_0's auc: 0.873279\n",
      "[1120]\tvalid_0's auc: 0.873587\n",
      "[1125]\tvalid_0's auc: 0.873641\n",
      "[1130]\tvalid_0's auc: 0.8737\n",
      "[1135]\tvalid_0's auc: 0.873815\n",
      "[1140]\tvalid_0's auc: 0.873774\n",
      "[1145]\tvalid_0's auc: 0.87379\n",
      "[1150]\tvalid_0's auc: 0.873965\n",
      "[1155]\tvalid_0's auc: 0.874153\n",
      "[1160]\tvalid_0's auc: 0.874314\n",
      "[1165]\tvalid_0's auc: 0.874255\n",
      "[1170]\tvalid_0's auc: 0.874311\n",
      "[1175]\tvalid_0's auc: 0.874359\n",
      "[1180]\tvalid_0's auc: 0.874455\n",
      "[1185]\tvalid_0's auc: 0.874677\n",
      "[1190]\tvalid_0's auc: 0.874642\n",
      "[1195]\tvalid_0's auc: 0.874856\n",
      "[1200]\tvalid_0's auc: 0.874997\n",
      "[1205]\tvalid_0's auc: 0.875035\n",
      "[1210]\tvalid_0's auc: 0.875125\n",
      "[1215]\tvalid_0's auc: 0.875213\n",
      "[1220]\tvalid_0's auc: 0.87526\n",
      "[1225]\tvalid_0's auc: 0.875328\n",
      "[1230]\tvalid_0's auc: 0.87548\n",
      "[1235]\tvalid_0's auc: 0.87559\n",
      "[1240]\tvalid_0's auc: 0.875662\n",
      "[1245]\tvalid_0's auc: 0.875694\n",
      "[1250]\tvalid_0's auc: 0.875802\n",
      "[1255]\tvalid_0's auc: 0.875804\n",
      "[1260]\tvalid_0's auc: 0.875818\n",
      "[1265]\tvalid_0's auc: 0.87597\n",
      "[1270]\tvalid_0's auc: 0.876103\n",
      "[1275]\tvalid_0's auc: 0.876341\n",
      "[1280]\tvalid_0's auc: 0.876403\n",
      "[1285]\tvalid_0's auc: 0.87647\n",
      "[1290]\tvalid_0's auc: 0.876441\n",
      "[1295]\tvalid_0's auc: 0.876519\n",
      "[1300]\tvalid_0's auc: 0.876673\n",
      "[1305]\tvalid_0's auc: 0.876681\n",
      "[1310]\tvalid_0's auc: 0.87678\n",
      "[1315]\tvalid_0's auc: 0.876915\n",
      "[1320]\tvalid_0's auc: 0.876926\n",
      "[1325]\tvalid_0's auc: 0.876963\n",
      "[1330]\tvalid_0's auc: 0.877054\n",
      "[1335]\tvalid_0's auc: 0.877213\n",
      "[1340]\tvalid_0's auc: 0.87727\n",
      "[1345]\tvalid_0's auc: 0.877497\n",
      "[1350]\tvalid_0's auc: 0.877599\n",
      "[1355]\tvalid_0's auc: 0.877736\n",
      "[1360]\tvalid_0's auc: 0.877685\n",
      "[1365]\tvalid_0's auc: 0.877725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1370]\tvalid_0's auc: 0.877791\n",
      "[1375]\tvalid_0's auc: 0.877911\n",
      "[1380]\tvalid_0's auc: 0.878015\n",
      "[1385]\tvalid_0's auc: 0.878041\n",
      "[1390]\tvalid_0's auc: 0.878166\n",
      "[1395]\tvalid_0's auc: 0.878378\n",
      "[1400]\tvalid_0's auc: 0.878383\n",
      "[1405]\tvalid_0's auc: 0.878313\n",
      "[1410]\tvalid_0's auc: 0.878301\n",
      "[1415]\tvalid_0's auc: 0.87825\n",
      "[1420]\tvalid_0's auc: 0.87831\n",
      "[1425]\tvalid_0's auc: 0.878464\n",
      "[1430]\tvalid_0's auc: 0.878594\n",
      "[1435]\tvalid_0's auc: 0.878663\n",
      "[1440]\tvalid_0's auc: 0.878723\n",
      "[1445]\tvalid_0's auc: 0.87883\n",
      "[1450]\tvalid_0's auc: 0.878876\n",
      "[1455]\tvalid_0's auc: 0.879056\n",
      "[1460]\tvalid_0's auc: 0.879003\n",
      "[1465]\tvalid_0's auc: 0.879147\n",
      "[1470]\tvalid_0's auc: 0.879163\n",
      "[1475]\tvalid_0's auc: 0.879321\n",
      "[1480]\tvalid_0's auc: 0.87947\n",
      "[1485]\tvalid_0's auc: 0.879575\n",
      "[1490]\tvalid_0's auc: 0.879728\n",
      "[1495]\tvalid_0's auc: 0.879784\n",
      "[1500]\tvalid_0's auc: 0.879766\n",
      "[1505]\tvalid_0's auc: 0.879753\n",
      "[1510]\tvalid_0's auc: 0.8798\n",
      "[1515]\tvalid_0's auc: 0.879861\n",
      "[1520]\tvalid_0's auc: 0.879966\n",
      "[1525]\tvalid_0's auc: 0.87999\n",
      "[1530]\tvalid_0's auc: 0.880019\n",
      "[1535]\tvalid_0's auc: 0.880087\n",
      "[1540]\tvalid_0's auc: 0.880161\n",
      "[1545]\tvalid_0's auc: 0.880308\n",
      "[1550]\tvalid_0's auc: 0.880392\n",
      "[1555]\tvalid_0's auc: 0.880405\n",
      "[1560]\tvalid_0's auc: 0.880484\n",
      "[1565]\tvalid_0's auc: 0.880456\n",
      "[1570]\tvalid_0's auc: 0.880542\n",
      "[1575]\tvalid_0's auc: 0.880646\n",
      "[1580]\tvalid_0's auc: 0.880676\n",
      "[1585]\tvalid_0's auc: 0.880864\n",
      "[1590]\tvalid_0's auc: 0.88094\n",
      "[1595]\tvalid_0's auc: 0.880957\n",
      "[1600]\tvalid_0's auc: 0.881028\n",
      "[1605]\tvalid_0's auc: 0.881106\n",
      "[1610]\tvalid_0's auc: 0.881169\n",
      "[1615]\tvalid_0's auc: 0.881312\n",
      "[1620]\tvalid_0's auc: 0.88138\n",
      "[1625]\tvalid_0's auc: 0.881595\n",
      "[1630]\tvalid_0's auc: 0.881715\n",
      "[1635]\tvalid_0's auc: 0.881657\n",
      "[1640]\tvalid_0's auc: 0.881607\n",
      "[1645]\tvalid_0's auc: 0.881663\n",
      "[1650]\tvalid_0's auc: 0.881699\n",
      "[1655]\tvalid_0's auc: 0.881828\n",
      "[1660]\tvalid_0's auc: 0.881874\n",
      "[1665]\tvalid_0's auc: 0.881967\n",
      "[1670]\tvalid_0's auc: 0.882188\n",
      "[1675]\tvalid_0's auc: 0.882268\n",
      "[1680]\tvalid_0's auc: 0.882198\n",
      "[1685]\tvalid_0's auc: 0.88226\n",
      "[1690]\tvalid_0's auc: 0.882476\n",
      "[1695]\tvalid_0's auc: 0.882498\n",
      "[1700]\tvalid_0's auc: 0.882579\n",
      "[1705]\tvalid_0's auc: 0.882617\n",
      "[1710]\tvalid_0's auc: 0.882754\n",
      "[1715]\tvalid_0's auc: 0.882801\n",
      "[1720]\tvalid_0's auc: 0.88292\n",
      "[1725]\tvalid_0's auc: 0.882909\n",
      "[1730]\tvalid_0's auc: 0.882932\n",
      "[1735]\tvalid_0's auc: 0.883042\n",
      "[1740]\tvalid_0's auc: 0.883058\n",
      "[1745]\tvalid_0's auc: 0.883231\n",
      "[1750]\tvalid_0's auc: 0.88329\n",
      "[1755]\tvalid_0's auc: 0.883365\n",
      "[1760]\tvalid_0's auc: 0.883404\n",
      "[1765]\tvalid_0's auc: 0.883388\n",
      "[1770]\tvalid_0's auc: 0.883348\n",
      "[1775]\tvalid_0's auc: 0.883441\n",
      "[1780]\tvalid_0's auc: 0.883549\n",
      "[1785]\tvalid_0's auc: 0.883577\n",
      "[1790]\tvalid_0's auc: 0.883608\n",
      "[1795]\tvalid_0's auc: 0.883545\n",
      "[1800]\tvalid_0's auc: 0.883559\n",
      "[1805]\tvalid_0's auc: 0.883628\n",
      "[1810]\tvalid_0's auc: 0.883722\n",
      "[1815]\tvalid_0's auc: 0.883727\n",
      "[1820]\tvalid_0's auc: 0.883754\n",
      "[1825]\tvalid_0's auc: 0.883846\n",
      "[1830]\tvalid_0's auc: 0.883843\n",
      "[1835]\tvalid_0's auc: 0.883834\n",
      "[1840]\tvalid_0's auc: 0.88393\n",
      "[1845]\tvalid_0's auc: 0.883998\n",
      "[1850]\tvalid_0's auc: 0.883932\n",
      "[1855]\tvalid_0's auc: 0.883948\n",
      "[1860]\tvalid_0's auc: 0.883971\n",
      "[1865]\tvalid_0's auc: 0.884021\n",
      "[1870]\tvalid_0's auc: 0.884082\n",
      "[1875]\tvalid_0's auc: 0.884133\n",
      "[1880]\tvalid_0's auc: 0.88415\n",
      "[1885]\tvalid_0's auc: 0.884155\n",
      "[1890]\tvalid_0's auc: 0.884162\n",
      "[1895]\tvalid_0's auc: 0.884262\n",
      "[1900]\tvalid_0's auc: 0.884329\n",
      "[1905]\tvalid_0's auc: 0.884336\n",
      "[1910]\tvalid_0's auc: 0.884394\n",
      "[1915]\tvalid_0's auc: 0.884406\n",
      "[1920]\tvalid_0's auc: 0.884396\n",
      "[1925]\tvalid_0's auc: 0.884405\n",
      "[1930]\tvalid_0's auc: 0.884456\n",
      "[1935]\tvalid_0's auc: 0.884464\n",
      "[1940]\tvalid_0's auc: 0.884518\n",
      "[1945]\tvalid_0's auc: 0.884539\n",
      "[1950]\tvalid_0's auc: 0.884585\n",
      "[1955]\tvalid_0's auc: 0.884637\n",
      "[1960]\tvalid_0's auc: 0.884734\n",
      "[1965]\tvalid_0's auc: 0.884843\n",
      "[1970]\tvalid_0's auc: 0.884913\n",
      "[1975]\tvalid_0's auc: 0.885007\n",
      "[1980]\tvalid_0's auc: 0.885066\n",
      "[1985]\tvalid_0's auc: 0.885025\n",
      "[1990]\tvalid_0's auc: 0.885108\n",
      "[1995]\tvalid_0's auc: 0.885136\n",
      "[2000]\tvalid_0's auc: 0.885167\n",
      "[2005]\tvalid_0's auc: 0.885189\n",
      "[2010]\tvalid_0's auc: 0.885249\n",
      "[2015]\tvalid_0's auc: 0.88527\n",
      "[2020]\tvalid_0's auc: 0.885375\n",
      "[2025]\tvalid_0's auc: 0.885426\n",
      "[2030]\tvalid_0's auc: 0.885517\n",
      "[2035]\tvalid_0's auc: 0.88557\n",
      "[2040]\tvalid_0's auc: 0.885652\n",
      "[2045]\tvalid_0's auc: 0.885642\n",
      "[2050]\tvalid_0's auc: 0.885673\n",
      "[2055]\tvalid_0's auc: 0.885711\n",
      "[2060]\tvalid_0's auc: 0.885739\n",
      "[2065]\tvalid_0's auc: 0.885808\n",
      "[2070]\tvalid_0's auc: 0.885813\n",
      "[2075]\tvalid_0's auc: 0.885873\n",
      "[2080]\tvalid_0's auc: 0.88589\n",
      "[2085]\tvalid_0's auc: 0.885927\n",
      "[2090]\tvalid_0's auc: 0.885952\n",
      "[2095]\tvalid_0's auc: 0.886018\n",
      "[2100]\tvalid_0's auc: 0.886023\n",
      "[2105]\tvalid_0's auc: 0.886102\n",
      "[2110]\tvalid_0's auc: 0.886075\n",
      "[2115]\tvalid_0's auc: 0.886104\n",
      "[2120]\tvalid_0's auc: 0.886092\n",
      "[2125]\tvalid_0's auc: 0.88611\n",
      "[2130]\tvalid_0's auc: 0.886116\n",
      "[2135]\tvalid_0's auc: 0.886088\n",
      "[2140]\tvalid_0's auc: 0.886128\n",
      "[2145]\tvalid_0's auc: 0.886158\n",
      "[2150]\tvalid_0's auc: 0.886271\n",
      "[2155]\tvalid_0's auc: 0.886328\n",
      "[2160]\tvalid_0's auc: 0.886332\n",
      "[2165]\tvalid_0's auc: 0.886339\n",
      "[2170]\tvalid_0's auc: 0.886432\n",
      "[2175]\tvalid_0's auc: 0.886448\n",
      "[2180]\tvalid_0's auc: 0.886469\n",
      "[2185]\tvalid_0's auc: 0.886507\n",
      "[2190]\tvalid_0's auc: 0.88654\n",
      "[2195]\tvalid_0's auc: 0.886661\n",
      "[2200]\tvalid_0's auc: 0.886666\n",
      "[2205]\tvalid_0's auc: 0.88672\n",
      "[2210]\tvalid_0's auc: 0.886782\n",
      "[2215]\tvalid_0's auc: 0.886778\n",
      "[2220]\tvalid_0's auc: 0.886757\n",
      "[2225]\tvalid_0's auc: 0.886796\n",
      "[2230]\tvalid_0's auc: 0.886804\n",
      "[2235]\tvalid_0's auc: 0.886879\n",
      "[2240]\tvalid_0's auc: 0.886882\n",
      "[2245]\tvalid_0's auc: 0.886937\n",
      "[2250]\tvalid_0's auc: 0.886985\n",
      "[2255]\tvalid_0's auc: 0.88705\n",
      "[2260]\tvalid_0's auc: 0.887012\n",
      "[2265]\tvalid_0's auc: 0.88704\n",
      "[2270]\tvalid_0's auc: 0.88709\n",
      "[2275]\tvalid_0's auc: 0.887106\n",
      "[2280]\tvalid_0's auc: 0.887138\n",
      "[2285]\tvalid_0's auc: 0.887221\n",
      "[2290]\tvalid_0's auc: 0.887256\n",
      "[2295]\tvalid_0's auc: 0.887263\n",
      "[2300]\tvalid_0's auc: 0.887255\n",
      "[2305]\tvalid_0's auc: 0.887285\n",
      "[2310]\tvalid_0's auc: 0.887324\n",
      "[2315]\tvalid_0's auc: 0.887404\n",
      "[2320]\tvalid_0's auc: 0.887407\n",
      "[2325]\tvalid_0's auc: 0.887387\n",
      "[2330]\tvalid_0's auc: 0.887322\n",
      "[2335]\tvalid_0's auc: 0.88736\n",
      "[2340]\tvalid_0's auc: 0.887389\n",
      "[2345]\tvalid_0's auc: 0.887414\n",
      "[2350]\tvalid_0's auc: 0.887411\n",
      "[2355]\tvalid_0's auc: 0.887463\n",
      "[2360]\tvalid_0's auc: 0.887504\n",
      "[2365]\tvalid_0's auc: 0.88753\n",
      "[2370]\tvalid_0's auc: 0.887527\n",
      "[2375]\tvalid_0's auc: 0.887558\n",
      "[2380]\tvalid_0's auc: 0.887591\n",
      "[2385]\tvalid_0's auc: 0.887584\n",
      "[2390]\tvalid_0's auc: 0.887611\n",
      "[2395]\tvalid_0's auc: 0.887598\n",
      "[2400]\tvalid_0's auc: 0.887614\n",
      "[2405]\tvalid_0's auc: 0.887562\n",
      "[2410]\tvalid_0's auc: 0.88759\n",
      "[2415]\tvalid_0's auc: 0.887648\n",
      "[2420]\tvalid_0's auc: 0.887665\n",
      "[2425]\tvalid_0's auc: 0.887703\n",
      "[2430]\tvalid_0's auc: 0.887732\n",
      "[2435]\tvalid_0's auc: 0.887792\n",
      "[2440]\tvalid_0's auc: 0.887823\n",
      "[2445]\tvalid_0's auc: 0.88784\n",
      "[2450]\tvalid_0's auc: 0.887852\n",
      "[2455]\tvalid_0's auc: 0.887859\n",
      "[2460]\tvalid_0's auc: 0.887857\n",
      "[2465]\tvalid_0's auc: 0.887939\n",
      "[2470]\tvalid_0's auc: 0.88794\n",
      "[2475]\tvalid_0's auc: 0.887907\n",
      "[2480]\tvalid_0's auc: 0.887907\n",
      "[2485]\tvalid_0's auc: 0.887912\n",
      "[2490]\tvalid_0's auc: 0.887913\n",
      "[2495]\tvalid_0's auc: 0.88794\n",
      "[2500]\tvalid_0's auc: 0.88792\n"
     ]
    }
   ],
   "source": [
    "#Train model on selected parameters and number of iterations\n",
    "lgbm = lgb.train(param,\n",
    "train_data,\n",
    "2500,\n",
    "valid_sets=valid_data,\n",
    "early_stopping_rounds= 500,\n",
    "verbose_eval= 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
